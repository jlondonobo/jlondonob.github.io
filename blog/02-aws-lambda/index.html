<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Building a quick and cost-effective data pipeline with AWS SDK for pandas and AWS Lambda | José Londoño</title>
<meta name=keywords content><meta name=description content="A guide on building quick and cost-effective pipelines in Python with AWS SDK for pandas and AWS Lambda."><meta name=author content><link rel=canonical href=https://joselondono.co/blog/02-aws-lambda/><link crossorigin=anonymous href=/assets/css/stylesheet.048c3c9ff5bc779bca15717002ee062e352b92a4925a91cab372d0817d4675c8.css integrity="sha256-BIw8n/W8d5vKFXFwAu4GLjUrkqSSWpHKs3LQgX1Gdcg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://joselondono.co/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://joselondono.co/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://joselondono.co/favicon-32x32.png><link rel=apple-touch-icon href=https://joselondono.co/apple-touch-icon.png><link rel=mask-icon href=https://joselondono.co/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Building a quick and cost-effective data pipeline with AWS SDK for pandas and AWS Lambda"><meta property="og:description" content="A guide on building quick and cost-effective pipelines in Python with AWS SDK for pandas and AWS Lambda."><meta property="og:type" content="article"><meta property="og:url" content="https://joselondono.co/blog/02-aws-lambda/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-01-21T00:00:00+00:00"><meta property="article:modified_time" content="2024-01-21T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Building a quick and cost-effective data pipeline with AWS SDK for pandas and AWS Lambda"><meta name=twitter:description content="A guide on building quick and cost-effective pipelines in Python with AWS SDK for pandas and AWS Lambda."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://joselondono.co/blog/"},{"@type":"ListItem","position":2,"name":"Building a quick and cost-effective data pipeline with AWS SDK for pandas and AWS Lambda","item":"https://joselondono.co/blog/02-aws-lambda/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Building a quick and cost-effective data pipeline with AWS SDK for pandas and AWS Lambda","name":"Building a quick and cost-effective data pipeline with AWS SDK for pandas and AWS Lambda","description":"A guide on building quick and cost-effective pipelines in Python with AWS SDK for pandas and AWS Lambda.","keywords":[],"articleBody":"Modern data pipelines are generally built on a dedicated server. Services like Airflow, Prefect, Mage, and Dagster manage data transfer and scheduling by constantly running on a server. This works fine for complex setups if there’s a large team to develop and maintain such infrastructure, and costs are not a problem. However, if you’re a solo developer or a small company, you might want to set up a simpler solution that you can deploy for virtually zero cost in a short period of time.\nThis article provides a guide on building a data pipeline that’s virtually free and which can be deployed to production in less than an hour. To build such a pipeline you’ll leverage the ease of use of AWS SDK for pandas(formerly AWS Data Wrangler) and the cost-effective runtime environment from AWS Lambda.\nThe article is divided in 2 sections:\nIntroduction to the setup: Introduces the tools you’ll use to build the pipeline. If you want to learn by doing, consider skipping this section. Building the pipeline: Goes on a detailed explanation of setting up the pipeline. TL;DR Using AWS SDK for pandas and Lambda functions together allows you to quickly and cost-effectively build data pipelines. However, AWS SDK for pandas and its dependencies exceed the size limits for a .zip Lambda deployment package.\nLambda layers effectively address this issue. AWS SDK for pandas managed layers offer a lightweight layer that contains both AWS SDK for pandas and pandas.\nFor your pipeline to function effectively, set a trigger and increase the function’s memory and timeout settings.\nFor the complete code, see aws-sdk-lambda-finance-demo.\nIntroduction to the setup Why use AWS SDK for pandas and Lambda functions to build a data pipeline Using AWS SDK for pandas and Lambda functions together allows you to quickly and cost-effectively build data pipelines. This setup is ideal for small or personal projects where speed and costs are limiting factors.\nAWS SDK for pandas streamlines interactions with AWS data services like Athena, Glue, RedShift, and S3. It eliminates the need for complex boto3 coding, simplifying ETL tasks such as reading and writing data from and to data lakes, warehouses, and databases.\nLambda functions provide a temporary runtime environment, eliminating the need for an EC2 instance setup and maintenance. Lambda functions also offer the following benefits\nEvent driven: They can run on a schedule (using cron), respond to file uploads, or HTTP requests. Cost-effective: Running a 30-second job on a 1 GB runtime twice daily costs about 3 cents, with a generous free tier. Quick to deploy: Allows for rapid development and deployment of pipelines, particularly effective with a CI/CD pipeline. A solution for using AWS SDK for pandas in Lambda functions AWS SDK for pandas and its dependencies exceed the size limits for a .zip Lambda deployment package. AWS Lambda sets a maximum size for deployment packages of 50 MB for compressed files and 250 MB for uncompressed files (uploaded to S3). However, the AWS SDK for pandas is 90 MB compressed and 268 MB uncompressed.\nLambda layers effectively address this issue. While alternatives like container image deployment packages exist, Lambda layers keep you within the .zip deployment package framework, offering advantages like the following:\nEase of use: Creating and deploying zip files is straightforward, especially for smaller Lambda functions. AWS Integration: Zip deployments integrate smoothly with AWS services like CodeBuild and the AWS SDK. I suggest using AWS SDK for pandas managed layers for simplicity and efficiency. They come pre-equipped with the following essential dependencies:\nAWS SDK for pandas Pandas Other minimal dependencies AWS SDK for pandas keeps layers updated for all active Python runtimes, regions, and architectures. For a detailed list of available layers, visit AWS Lambda Managed Layers.\nEnd-to-end pipeline This section offers a step-by-step guide on building a data pipeline with AWS SDK for pandas and AWS lambda with an application for finance. Despite its focus on finance, the pipeline can be customized to call any API.\nThe pipeline involves the following steps:\nCall a simulated financial API. Store the API’s response in AWS Glue. By the end of this tutorial, your deployment package should look like this:\naws_sdk_lambda_finance_demo ├── api │ ├── __init__.py │ └── finance.py └── lambda_function.py This deployment package doesn’t use external dependencies. However, many real world applications do. If you’d like to add external dependencies to your deployment package, see Creating a .zip deployment package with dependencies.\nFor the complete code, see aws-sdk-lambda-finance-demo.\nNote: Implementing this pipeline might result in AWS charges if your free-tier trial has expired. Ensure to deactivate any services you enable for this tutorial.\nStep 1: Write the lambda function The first step to building the pipeline is to actually write it. Write your own pipeline or use the following sample to get started:\n# aws_sdk_lambda_finance_demo/lambda_function.py import awswrangler as wr import pandas as pd from api import finance def create_database_if_not_exists(database_name): # Check if the database exists databases = wr.catalog.get_databases() if not any(db[\"Name\"] == database_name for db in databases): wr.catalog.create_database(database_name) def lambda_handler(event, context): # Fetch data from the API daily_stocks = finance.call_api() # Convert the response to a DataFrame stocks = pd.DataFrame(daily_stocks) # Define your database name database_name = \"market_tracker\" # Create the database if it doesn't exist create_database_if_not_exists(database_name) # Define the S3 path where you want to store the data s3_path = \"s3://acme/glue/market_tracker/daily_stocks\" table_name = \"daily_stocks\" # Write the DataFrame to the Glue table, appending to the existing table wr.s3.to_parquet( df=stocks, path=s3_path, dataset=True, database=database_name, table=table_name, mode=\"append\" ) Note the absence of credentials in the lambda_function.py file. If you’re following along with the sample files, ensure the AWS CLI is set up on your system . If you haven’t configured the AWS CLI, see Setting up the AWS CLI .\n# aws_sdk_lambda_finance_demo/api/finance.py import random import datetime def call_api(): # Fixed set of three tickers tickers = [\"AAPL\", \"MSFT\", \"TSLA\"] # Generate data for each ticker data_collection = [] for ticker in tickers: data = { \"date\": datetime.date.today().isoformat(), \"ticker\": ticker, \"stock_price\": round(random.uniform(100, 500), 2), \"trading_volume\": random.randint(1000, 10000), } data_collection.append(data) return data_collection Step 2: Create the lambda function on the AWS console Once you wrote the pipeline, go ahead and create a Lambda function on the AWS console. To create a function, take the following steps:\nGo to AWS Lambda console. Locate the Functions tab. Click on Create function. Select Author from scratch. Name the function. Select a Python runtime. Click on Create function. Before choosing a runtime, make sure there’s a managed layer for it. AWS SDK for pandas’ managed layers do not support all runtimes. If you select a runtime for which managed layers are not available, you won’t be able to attach the layer.\n![[images/01_create_function.png]]\n![[images/02_function_setup.png]]\nStep 3: Update the function’s configuration and execution role The default state of Lambda functions is not ideal for running data pipelines. Memory is capped at 128 MB, the timeout set to 3 seconds and the function has no permissions to interact with other AWS services. With this configuration, any pipeline that uses more than 128 MB of memory, takes longer than 3 seconds or interacts with other AWS services will be cancelled.\nConsequently, for the function to run you must update its runtime’s configuration. Here are recommended values for memory and timeout:\nMemory: AWS SDK for pandas suggest assigning at least 512 MB of memory . Tune it up if you expect to move large amounts of data. I wouldn’t recommend tuning it down, as it can make even small functions fail unexpectedly. Timeout: Set the timeout parameter to at least 1.5x-2x the average execution time of your pipeline. If your function’s execution times vary significantly, set a higher value. For example, if your pipeline usually takes 30 seconds to complete, you should set the timeout parameter to at least 45-60 seconds. If your function interacts with other AWS resources like Glue, S3 or Redshift, you must update the function’s execution role. Customizing the execution role of a Lambda function is outside the scope of this tutorial. However, the following tips can hep you build your own\nIAM Roles Tutorial by Stephane Maarek. Example Lambda execution role. Tip: Describe the pipeline to ChatGPT and ask it to generate an AWS JSON policy. ![[images/03_start_config.png]]\n![[images/04_edit_config.png]]\nStep 4: Add the AWS SDK for pandas managed layer Once you correctly configured the function, add the AWS SDK for pandas managed function. To add the function, take the following steps:\nLocate the Code tab. Click on the Add layer button, located in the Layers panel. Select the AWSSDKPandas layer and its version from the dropdown menus. Click on Add. ![[images/05_layers_start.png]] ![[images/06_layers_config.png]]\nIf you successfully added the layer, the “Layers” diagram element will show a (1) next to it. ![[images/07_layers_confirmation.png]]\nStep 5: Set up a cron trigger In the final step of the configuration, you’ll set up the scheduler. Namely, an EventBridge cron trigger event. This trigger will run the function at pre-specified times.\nCron statements are minute-precise. They have control of running a program at a minute level, but not at a second level. If you need second-level control, start the function a minute earlier, and write the starting logic in the code.\nTo set up a cron trigger, take the following steps: 2. Click on the Add trigger. 3. Select EventrBridge trigger. 4. Select Create a new rule. 5. Name the trigger and add a description. 6. Select Scheduled expression. 7. Define the function’s execution schedule using cron notation. 8. Click on Add. ![[images/08_add_trigger.png]]\n![[images/09_configure_trigger.png]] EventBridge cron statements differ from traditional Unix-style cron statements. If you’re unfamiliar with its syntax or want a refresher, ask ChatGPT to generate the statements for you. Take the following differences into consideration:\nEventBridge statements include a year field. EventBridge statements can use the ? wildcard. EventBridge statements are in UTC. Step 6: Upload the deployment package The last step of the setup is to upload the deployment package. To do so, take the following steps:\nCompress the api/ and lambda_function.py into a .zip file. Locate the Code tab. Click the Upload from dropdown. Select .zip file. Drag and drop your deployment package. Click on save. ![[images/10_upload_zip.png]] ![[images/11_drag_zip.png]]\nStep 7: Test the pipeline The pipeline is now online. Test it by taking the following steps:\nGo to the Test tab and hit the Test button. Go to the AWS Athena console and query the table you just created. ![[images/12_test_function.png]]\n","wordCount":"1717","inLanguage":"en","datePublished":"2024-01-21T00:00:00Z","dateModified":"2024-01-21T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://joselondono.co/blog/02-aws-lambda/"},"publisher":{"@type":"Organization","name":"José Londoño","logo":{"@type":"ImageObject","url":"https://joselondono.co/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Building a quick and cost-effective data pipeline with AWS SDK for pandas and AWS Lambda</h1><div class=post-description>A guide on building quick and cost-effective pipelines in Python with AWS SDK for pandas and AWS Lambda.</div><div class=post-meta>&lt;span title='2024-01-21 00:00:00 +0000 UTC'>January 21, 2024&lt;/span>&amp;nbsp;·&amp;nbsp;9 min</div></header><div class=post-content><p>Modern data pipelines are generally built on a dedicated server. Services like Airflow, Prefect, Mage, and Dagster manage data transfer and scheduling by constantly running on a server. This works fine for complex setups if there&rsquo;s a large team to develop and maintain such infrastructure, and costs are not a problem. However, if you&rsquo;re a solo developer or a small company, you might want to set up a simpler solution that you can deploy for virtually zero cost in a short period of time.</p><p>This article provides a guide on building a data pipeline that&rsquo;s virtually free and which can be deployed to production in less than an hour. To build such a pipeline you&rsquo;ll leverage the ease of use of <strong><a href=https://aws-sdk-pandas.readthedocs.io/en/stable/index.html>AWS SDK for pandas</a></strong>(formerly AWS Data Wrangler) and the cost-effective runtime environment from <strong>AWS Lambda</strong>.</p><p>The article is divided in 2 sections:</p><ul><li><a href>Introduction to the setup</a>: Introduces the tools you&rsquo;ll use to build the pipeline. If you want to learn by doing, consider skipping this section.</li><li><a href>Building the pipeline</a>: Goes on a detailed explanation of setting up the pipeline.</li></ul><h2 id=tldr>TL;DR<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h2><p>Using <strong>AWS SDK for pandas</strong> and <strong>Lambda functions</strong> together allows you to <strong>quickly</strong> and <strong>cost-effectively</strong> build data pipelines. However, AWS SDK for pandas and its dependencies exceed the size limits for a <strong>.zip Lambda deployment package</strong>.</p><p><strong>Lambda layers</strong> effectively address this issue. <a href=https://aws-sdk-pandas.readthedocs.io/en/stable/layers.html>AWS SDK for pandas managed layers</a> offer a lightweight layer that contains both AWS SDK for pandas and pandas.</p><p>For your pipeline to function effectively, set a trigger and increase the function&rsquo;s <strong>memory</strong> and <strong>timeout</strong> settings.</p><p>For the complete code, see <a href=https://github.com/jlondonobo/aws-sdk-lambda-finance-demo>aws-sdk-lambda-finance-demo</a>.</p><h2 id=introduction-to-the-setup>Introduction to the setup<a hidden class=anchor aria-hidden=true href=#introduction-to-the-setup>#</a></h2><h3 id=why-use-aws-sdk-for-pandas-and-lambda-functions-to-build-a-data-pipeline>Why use AWS SDK for pandas and Lambda functions to build a data pipeline<a hidden class=anchor aria-hidden=true href=#why-use-aws-sdk-for-pandas-and-lambda-functions-to-build-a-data-pipeline>#</a></h3><p>Using <strong>AWS SDK for pandas</strong> and <strong>Lambda functions</strong> together allows you to <strong>quickly</strong> and <strong>cost-effectively</strong> build data pipelines. This setup is ideal for small or personal projects where speed and costs are limiting factors.</p><p><strong>AWS SDK for pandas</strong> streamlines interactions with <strong>AWS data services</strong> like Athena, Glue, RedShift, and S3. It eliminates the need for complex <code>boto3</code> coding, simplifying ETL tasks such as reading and writing data from and to data lakes, warehouses, and databases.</p><p><strong>Lambda functions</strong> provide a temporary runtime environment, eliminating the need for an EC2 instance setup and maintenance. Lambda functions also offer the following benefits</p><ul><li><strong>Event driven</strong>: They can run on a schedule (using cron), respond to file uploads, or HTTP requests.</li><li><strong>Cost-effective</strong>: Running a 30-second job on a 1 GB runtime twice daily costs about 3 cents, with a generous free tier.</li><li><strong>Quick to deploy</strong>: Allows for rapid development and deployment of pipelines, particularly effective with a <a href>CI/CD pipeline</a>.</li></ul><h3 id=a-solution-for-using-aws-sdk-for-pandas-in-lambda-functions>A solution for using AWS SDK for pandas in Lambda functions<a hidden class=anchor aria-hidden=true href=#a-solution-for-using-aws-sdk-for-pandas-in-lambda-functions>#</a></h3><p>AWS SDK for pandas and its dependencies exceed the size limits for a <strong>.zip Lambda deployment package</strong>. AWS Lambda sets a maximum size for deployment packages of <strong>50 MB for compressed files</strong> and <strong>250 MB for uncompressed files</strong> (uploaded to S3). However, the AWS SDK for pandas is 90 MB compressed and 268 MB uncompressed.</p><p><strong>Lambda layers</strong> effectively address this issue. While alternatives like <strong>container image deployment packages</strong> exist, Lambda layers keep you within the <strong>.zip deployment package</strong> framework, offering advantages like the following:</p><ul><li><strong>Ease of use</strong>: Creating and deploying zip files is straightforward, especially for smaller Lambda functions.</li><li><strong>AWS Integration</strong>: Zip deployments integrate smoothly with AWS services like CodeBuild and the AWS SDK.</li></ul><p>I suggest using <a href=https://aws-sdk-pandas.readthedocs.io/en/stable/layers.html>AWS SDK for pandas managed layers</a> for simplicity and efficiency. They come pre-equipped with the following essential dependencies:</p><ul><li>AWS SDK for pandas</li><li>Pandas</li><li>Other minimal dependencies</li></ul><p>AWS SDK for pandas keeps layers updated for all active Python runtimes, regions, and architectures. For a detailed list of available layers, visit <a href=https://aws-sdk-pandas.readthedocs.io/en/stable/layers.html>AWS Lambda Managed Layers</a>.</p><h2 id=end-to-end-pipeline>End-to-end pipeline<a hidden class=anchor aria-hidden=true href=#end-to-end-pipeline>#</a></h2><p>This section offers a step-by-step guide on building a data pipeline with AWS SDK for pandas and AWS lambda with an application for finance. Despite its focus on finance, the pipeline can be customized to call any API.</p><p>The pipeline involves the following steps:</p><ol><li>Call a simulated financial API.</li><li>Store the API&rsquo;s response in AWS Glue.</li></ol><p>By the end of this tutorial, your deployment package should look like this:</p><pre tabindex=0><code>aws_sdk_lambda_finance_demo
├── api
│   ├── __init__.py
│   └── finance.py
└── lambda_function.py
</code></pre><p>This deployment package <strong>doesn&rsquo;t use external dependencies</strong>. However, many real world applications do. If you&rsquo;d like to add external dependencies to your deployment package, see <a href=https://docs.aws.amazon.com/lambda/latest/dg/python-package.html#python-package-create-dependencies>Creating a .zip deployment package with dependencies</a>.</p><p>For the complete code, see <a href=https://github.com/jlondonobo/aws-sdk-lambda-finance-demo>aws-sdk-lambda-finance-demo</a>.</p><p>Note: Implementing this pipeline might result in AWS charges if your free-tier trial has expired. Ensure to deactivate any services you enable for this tutorial.</p><h3 id=step-1-write-the-lambda-function>Step 1: Write the lambda function<a hidden class=anchor aria-hidden=true href=#step-1-write-the-lambda-function>#</a></h3><p>The first step to building the pipeline is to actually write it. Write your own pipeline or use the following sample to get started:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># aws_sdk_lambda_finance_demo/lambda_function.py</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> awswrangler <span style=color:#66d9ef>as</span> wr
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> api <span style=color:#f92672>import</span> finance
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>create_database_if_not_exists</span>(database_name):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Check if the database exists</span>
</span></span><span style=display:flex><span>    databases <span style=color:#f92672>=</span> wr<span style=color:#f92672>.</span>catalog<span style=color:#f92672>.</span>get_databases()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> any(db[<span style=color:#e6db74>&#34;Name&#34;</span>] <span style=color:#f92672>==</span> database_name <span style=color:#66d9ef>for</span> db <span style=color:#f92672>in</span> databases):
</span></span><span style=display:flex><span>        wr<span style=color:#f92672>.</span>catalog<span style=color:#f92672>.</span>create_database(database_name)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>lambda_handler</span>(event, context):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Fetch data from the API</span>
</span></span><span style=display:flex><span>    daily_stocks <span style=color:#f92672>=</span> finance<span style=color:#f92672>.</span>call_api()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Convert the response to a DataFrame</span>
</span></span><span style=display:flex><span>    stocks <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(daily_stocks)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Define your database name</span>
</span></span><span style=display:flex><span>    database_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;market_tracker&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create the database if it doesn&#39;t exist</span>
</span></span><span style=display:flex><span>    create_database_if_not_exists(database_name)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Define the S3 path where you want to store the data</span>
</span></span><span style=display:flex><span>    s3_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;s3://acme/glue/market_tracker/daily_stocks&#34;</span>
</span></span><span style=display:flex><span>    table_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;daily_stocks&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Write the DataFrame to the Glue table, appending to the existing table</span>
</span></span><span style=display:flex><span>    wr<span style=color:#f92672>.</span>s3<span style=color:#f92672>.</span>to_parquet(
</span></span><span style=display:flex><span>        df<span style=color:#f92672>=</span>stocks,
</span></span><span style=display:flex><span>        path<span style=color:#f92672>=</span>s3_path,
</span></span><span style=display:flex><span>        dataset<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        database<span style=color:#f92672>=</span>database_name,
</span></span><span style=display:flex><span>        table<span style=color:#f92672>=</span>table_name,
</span></span><span style=display:flex><span>        mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;append&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div><p>Note the absence of credentials in the <code>lambda_function.py</code> file. If you&rsquo;re following along with the sample files, ensure the <strong>AWS CLI</strong> is set up on your system . If you haven&rsquo;t configured the AWS CLI, see <a href=https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html>Setting up the AWS CLI</a> .</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># aws_sdk_lambda_finance_demo/api/finance.py</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> datetime
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>call_api</span>():
</span></span><span style=display:flex><span>    <span style=color:#75715e># Fixed set of three tickers</span>
</span></span><span style=display:flex><span>    tickers <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;AAPL&#34;</span>, <span style=color:#e6db74>&#34;MSFT&#34;</span>, <span style=color:#e6db74>&#34;TSLA&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Generate data for each ticker</span>
</span></span><span style=display:flex><span>    data_collection <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> ticker <span style=color:#f92672>in</span> tickers:
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;date&#34;</span>: datetime<span style=color:#f92672>.</span>date<span style=color:#f92672>.</span>today()<span style=color:#f92672>.</span>isoformat(),
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;ticker&#34;</span>: ticker,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;stock_price&#34;</span>: round(random<span style=color:#f92672>.</span>uniform(<span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>500</span>), <span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trading_volume&#34;</span>: random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>1000</span>, <span style=color:#ae81ff>10000</span>),
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        data_collection<span style=color:#f92672>.</span>append(data)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> data_collection
</span></span></code></pre></div><h3 id=step-2-create-the-lambda-function-on-the-aws-console>Step 2: Create the lambda function on the AWS console<a hidden class=anchor aria-hidden=true href=#step-2-create-the-lambda-function-on-the-aws-console>#</a></h3><p>Once you wrote the pipeline, go ahead and create a Lambda function on the AWS console. To create a function, take the following steps:</p><ol><li>Go to <strong>AWS Lambda console</strong>.</li><li>Locate the <strong>Functions tab</strong>.</li><li>Click on <strong>Create function</strong>.</li><li>Select <strong>Author from scratch</strong>.</li><li>Name the function.</li><li>Select a <strong>Python runtime</strong>.</li><li>Click on <strong>Create function</strong>.</li></ol><p>Before choosing a runtime, make sure there&rsquo;s a <a href=https://aws-sdk-pandas.readthedocs.io/en/stable/layers.html>managed layer</a> for it. AWS SDK for pandas&rsquo; managed layers do not support all runtimes. If you select a runtime for which managed layers are not available, you won&rsquo;t be able to attach the layer.</p><p>![[images/01_create_function.png]]</p><p>![[images/02_function_setup.png]]</p><h4 id=step-3-update-the-functions-configuration-and-execution-role>Step 3: Update the function&rsquo;s configuration and execution role<a hidden class=anchor aria-hidden=true href=#step-3-update-the-functions-configuration-and-execution-role>#</a></h4><p>The default state of Lambda functions is not ideal for running data pipelines. Memory is capped at 128 MB, the timeout set to 3 seconds and the function has no permissions to interact with other AWS services. With this configuration, any pipeline that uses more than 128 MB of memory, takes longer than 3 seconds or interacts with other AWS services will be cancelled.</p><p>Consequently, for the function to run you must update its runtime&rsquo;s configuration. Here are recommended values for memory and timeout:</p><ul><li><strong>Memory</strong>: AWS SDK for pandas <a href=https://aws-sdk-pandas.readthedocs.io/en/stable/install.html#aws-lambda-layer>suggest assigning at least 512 MB of memory</a> . Tune it up if you expect to move large amounts of data. I wouldn&rsquo;t recommend tuning it down, as it can make even small functions fail unexpectedly.</li><li><strong>Timeout</strong>: Set the timeout parameter to at least 1.5x-2x the average execution time of your pipeline. If your function&rsquo;s execution times vary significantly, set a higher value. For example, if your pipeline usually takes 30 seconds to complete, you should set the timeout parameter to at least 45-60 seconds.</li></ul><p>If your function interacts with other AWS resources like Glue, S3 or Redshift, you must update the function&rsquo;s <strong>execution role</strong>. Customizing the execution role of a Lambda function is outside the scope of this tutorial. However, the following tips can hep you build your own</p><ul><li><a href="https://www.youtube.com/watch?v=lAkadozQwdo">IAM Roles Tutorial</a> by <a href=https://www.youtube.com/@StephaneMaarek>Stephane Maarek</a>.</li><li><a href=https://github.com/jlondonobo/aws-sdk-lambda-finance-demo/blob/main/role_example.json>Example Lambda execution role</a>.</li><li>Tip: Describe the pipeline to ChatGPT and ask it to generate an AWS JSON policy.</li></ul><p>![[images/03_start_config.png]]</p><p>![[images/04_edit_config.png]]</p><h4 id=step-4-add-the-aws-sdk-for-pandas-managed-layer>Step 4: Add the AWS SDK for pandas managed layer<a hidden class=anchor aria-hidden=true href=#step-4-add-the-aws-sdk-for-pandas-managed-layer>#</a></h4><p>Once you correctly configured the function, add the AWS SDK for pandas managed function. To add the function, take the following steps:</p><ol><li>Locate the <strong>Code</strong> tab.</li><li>Click on the <strong>Add layer</strong> button, located in the <strong>Layers</strong> panel.</li><li>Select the <strong>AWSSDKPandas layer</strong> and its <strong>version</strong> from the dropdown menus.</li><li>Click on <strong>Add</strong>.</li></ol><p>![[images/05_layers_start.png]]
![[images/06_layers_config.png]]</p><p>If you successfully added the layer, the <strong>&ldquo;Layers&rdquo; diagram element</strong> will show a (1) next to it.
![[images/07_layers_confirmation.png]]</p><h3 id=step-5-set-up-a-cron-trigger>Step 5: Set up a cron trigger<a hidden class=anchor aria-hidden=true href=#step-5-set-up-a-cron-trigger>#</a></h3><p>In the final step of the configuration, you&rsquo;ll set up the scheduler. Namely, an EventBridge <strong>cron trigger event</strong>. This trigger will run the function at pre-specified times.</p><p>Cron statements are minute-precise. They have control of running a program at a minute level, but not at a second level. If you need second-level control, start the function a minute earlier, and write the starting logic in the code.</p><p>To set up a cron trigger, take the following steps: 2. Click on the <strong>Add trigger</strong>. 3. Select <strong>EventrBridge trigger</strong>. 4. Select <strong>Create a new rule</strong>. 5. Name the trigger and add a description. 6. Select <strong>Scheduled expression</strong>. 7. Define the function&rsquo;s execution schedule using cron notation. 8. Click on <strong>Add</strong>.
![[images/08_add_trigger.png]]</p><p>![[images/09_configure_trigger.png]]
EventBridge cron statements differ from traditional Unix-style cron statements. If you&rsquo;re unfamiliar with its syntax or want a refresher, ask ChatGPT to generate the statements for you. Take the following differences into consideration:</p><ul><li>EventBridge statements include a year field.</li><li>EventBridge statements can use the ? wildcard.</li><li>EventBridge statements are in UTC.</li></ul><h3 id=step-6-upload-the-deployment-package>Step 6: Upload the deployment package<a hidden class=anchor aria-hidden=true href=#step-6-upload-the-deployment-package>#</a></h3><p>The last step of the setup is to upload the <em>deployment package</em>. To do so, take the following steps:</p><ol><li>Compress the <code>api/</code> and <code>lambda_function.py</code> into a .zip file.</li><li>Locate the <strong>Code</strong> tab.</li><li>Click the <strong>Upload from</strong> dropdown.</li><li>Select <strong>.zip file</strong>.</li><li>Drag and drop your deployment package.</li><li>Click on <strong>save</strong>.</li></ol><p>![[images/10_upload_zip.png]]
![[images/11_drag_zip.png]]</p><h3 id=step-7-test-the-pipeline>Step 7: Test the pipeline<a hidden class=anchor aria-hidden=true href=#step-7-test-the-pipeline>#</a></h3><p>The pipeline is now online. Test it by taking the following steps:</p><ol><li>Go to the <strong>Test</strong> tab and hit the <strong>Test</strong> button.</li><li>Go to the <strong>AWS Athena console</strong> and query the table you just created.</li></ol><p>![[images/12_test_function.png]]</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://joselondono.co/>José Londoño</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>